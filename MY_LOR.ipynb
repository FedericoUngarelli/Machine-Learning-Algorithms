{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(lst):\n",
    "    \"\"\"\n",
    "        Helper function for movielens dataset, not useful for discrete multi class clasification.\n",
    "\n",
    "        Return:\n",
    "        Normalized list x, in range [0, 1]\n",
    "    \"\"\"\n",
    "    maximum = max(lst)\n",
    "    minimum = min(lst)\n",
    "    toreturn = []\n",
    "    for i in range(len(lst)):\n",
    "        toreturn.append((lst[i]- minimum)/ (maximum - minimum))\n",
    "    return toreturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_standardize(X_inp):\n",
    "    \"\"\"\n",
    "        Z-score Standardization.\n",
    "        Standardize the feature matrix, and store the standarize rule.\n",
    "\n",
    "        Parameter:\n",
    "        X_inp: Input feature matrix.\n",
    "\n",
    "        Return:\n",
    "        Standardized feature matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    toreturn = X_inp.copy()\n",
    "    for i in range(X_inp.shape[1]):\n",
    "        std = np.std(X_inp[:, i])               # ------ Find the standard deviation of the feature\n",
    "        mean = np.mean(X_inp[:, i])             # ------ Find the mean value of the feature\n",
    "        temp = (X_inp[:, i] - mean) / std\n",
    "        \n",
    "        \"\"\"\n",
    "        for j in np.array(X_inp[:, i]):\n",
    "            \n",
    "           \n",
    "                #TODO: 1. implement the standardize function\n",
    "            \n",
    "        \n",
    "            temp += []\n",
    "        \"\"\"\n",
    "        toreturn[:, i] = temp\n",
    "    return toreturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" \n",
    "        Sigmoid Function\n",
    "\n",
    "        Return:\n",
    "        transformed x.\n",
    "    \"\"\"\n",
    "    \"\"\"    \n",
    "        #TODO: 2. implement the sigmoid function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Some initializations, if neccesary\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model_name = 'Logistic Regression'\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "            Save the datasets in our model, and do normalization to y_train\n",
    "            \n",
    "            Parameter:\n",
    "                X_train: Matrix or 2-D array. Input feature matrix.\n",
    "                Y_train: Matrix or 2-D array. Input target value.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X = X_train\n",
    "        self.y = y_train\n",
    "        \n",
    "        count = 0\n",
    "        uni = np.unique(y_train)\n",
    "        for y in y_train:\n",
    "            if y == min(uni):\n",
    "                self.y[count] = 0\n",
    "            else:\n",
    "                self.y[count] = 1\n",
    "            count += 1        \n",
    "        \n",
    "        n, m = X_train.shape\n",
    "        self.theta = np.zeros(m)\n",
    "        self.b = 0\n",
    "    \n",
    "    def gradient(self, X_inp, y_inp, theta, b):\n",
    "        \"\"\"\n",
    "            Calculate the grandient of Weight and Bias, given sigmoid_yhat, true label, and data\n",
    "\n",
    "            Parameter:\n",
    "                X_inp: Matrix or 2-D array. Input feature matrix.\n",
    "                y_inp: Matrix or 2-D array. Input target value.\n",
    "                theta: Matrix or 1-D array. Weight matrix.\n",
    "                b: int. Bias.\n",
    "\n",
    "            Return:\n",
    "                grad_theta: gradient with respect to theta\n",
    "                grad_b: gradient with respect to b\n",
    "\n",
    "        NOTE: There are several ways of implementing the gradient. We are merely providing you one way\n",
    "        of doing it. Feel free to change the code and implement the way you want.\n",
    "        \"\"\"\n",
    "        grad_b = b\n",
    "        grad_theta = np.zeros_like(theta)\n",
    "\n",
    "        for (xi, yi) in zip(X_inp, y_inp):\n",
    "            z = np.dot(theta, xi) + b\n",
    "            sigmoid_yhat = sigmoid(z)\n",
    "            grad_b += (sigmoid_yhat - yi)\n",
    "            grad_theta += (sigmoid_yhat - yi) * xi\n",
    "        \n",
    "        grad_b /= len(X_inp)\n",
    "        grad_theta /= len(X_inp)\n",
    "    \n",
    "        return grad_theta, grad_b\n",
    "\n",
    "    def gradient_descent_logistic(self, alpha, num_pass, early_stop=0, standardized = True):\n",
    "        \"\"\"\n",
    "            Logistic Regression with gradient descent method\n",
    "\n",
    "            Parameter:\n",
    "                alpha: (Hyper Parameter) Learning rate.\n",
    "                num_pass: Number of iteration\n",
    "                early_stop: (Hyper Parameter) Least improvement error allowed before stop. \n",
    "                            If improvement is less than the given value, then terminate the function and store the coefficents.\n",
    "                            default = 0.\n",
    "                standardized: bool, determine if we standardize the feature matrix.\n",
    "                \n",
    "            Return:\n",
    "                self.theta: theta after training\n",
    "                self.b: b after training\n",
    "        \"\"\"\n",
    "        \n",
    "        if standardized:\n",
    "            self.X = z_standardize(self.X)\n",
    "        \n",
    "        n, m = self.X.shape\n",
    "\n",
    "        for i in range(num_pass):    \n",
    "            \n",
    "            \"\"\"\n",
    "                TODO: 4. Modify the following code to implement gradient descent algorithm\n",
    "            \"\"\"\n",
    "            grad_theta, grad_b = self.gradient(self.X, self.y, self.theta, self.b)\n",
    "            temp_theta = self.theta - alpha * grad_theta\n",
    "            temp_b = self.b - alpha * grad_b\n",
    "\n",
    "            \"\"\"\n",
    "                TODO: 5. Modify the following code to implement early Stop Mechanism (use Logistic Loss when calculating error)\n",
    "            \"\"\" \n",
    "            \n",
    "            previous_y_hat = sigmoid(np.dot(self.X, self.theta) + self.b)\n",
    "            temp_y_hat = sigmoid(np.dot(self.X, temp_theta) + temp_b)\n",
    "            pre_error = - np.mean(self.y * np.log(previous_y_hat) + (1 - self.y) * np.log(1 - previous_y_hat))\n",
    "            temp_error = - np.mean(self.y * np.log(temp_y_hat) + (1 - self.y) * np.log(1 - temp_y_hat))\n",
    "            #print(f'temp err: {temp_error}')\n",
    "            #print(f'pre err: {pre_error}')\n",
    "            #print(f'ratio: {abs(abs(temp_error - pre_error) / pre_error)}')\n",
    "\n",
    "            if (abs(temp_error - pre_error) < early_stop) | (abs(abs(temp_error - pre_error) / pre_error) < early_stop):\n",
    "                print('early stop')\n",
    "                return temp_theta, temp_b\n",
    "\n",
    "            #pre_error = temp_error\n",
    "            #previous_y_hat = temp_y_hat\n",
    "            self.theta = temp_theta\n",
    "            self.b = temp_b\n",
    "\n",
    "        return self.theta, self.b\n",
    "\n",
    "    \n",
    "    def predict_ind(self, x: list):\n",
    "        \"\"\"\n",
    "            Predict the most likely class label of one test instance based on its feature vector x.\n",
    "\n",
    "            Parameter:\n",
    "            x: Matrix, array or list. Input feature point.\n",
    "            \n",
    "            Return:\n",
    "                p: prediction of given data point\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "            TODO: 7. Implement the prediction function\n",
    "        \"\"\"\n",
    "        x = np.array(x)\n",
    "        z = np.dot(self.theta, x) + self.b\n",
    "        p = sigmoid(z)\n",
    "        return p\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Parameter:\n",
    "            x: Matrix, array or list. Input feature point.\n",
    "            \n",
    "            Return:\n",
    "                p: prediction of given data matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "            TODO: 8. Revise the following for-loop to call predict_ind to get predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        ret = []                  # -------- Use predict_ind to generate the prediction list\n",
    "        for x in X:\n",
    "            # call predict_ind to get prediction for each instance\n",
    "            pred = self.predict_ind(x)\n",
    "            ret.append(pred)\n",
    "    \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "#names = ['f_acid', 'v_acid', 'c_acid', 'sugar', 'chlorides', 'f_SO2', 't_SO2', 'density', 'ph', 'sulphates', 'alcohol', 'quality']\n",
    "wine = pd.read_csv(url_Wine, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.068</td>\n",
       "      <td>28.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.99651</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.82</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1319 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4              0.70         0.00             1.9      0.076   \n",
       "1               7.8              0.88         0.00             2.6      0.098   \n",
       "2               7.8              0.76         0.04             2.3      0.092   \n",
       "4               7.4              0.70         0.00             1.9      0.076   \n",
       "5               7.4              0.66         0.00             1.8      0.075   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1592            6.3              0.51         0.13             2.3      0.076   \n",
       "1593            6.8              0.62         0.08             1.9      0.068   \n",
       "1595            5.9              0.55         0.10             2.2      0.062   \n",
       "1596            6.3              0.51         0.13             2.3      0.076   \n",
       "1598            6.0              0.31         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "5                    13.0                  40.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1592                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1593                 28.0                  38.0  0.99651  3.42       0.82   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "4         9.4        5  \n",
       "5         9.4        5  \n",
       "...       ...      ...  \n",
       "1592     11.0        6  \n",
       "1593      9.5        6  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1319 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine5 = wine.loc[wine.quality == 5]\n",
    "wine6 = wine.loc[wine.quality == 6]\n",
    "wineall = pd.concat([wine5,wine6])\n",
    "wineall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(wineall.iloc[:,:10])\n",
    "Y = np.array(wineall.quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for y in Y:\n",
    "    if y == 5:\n",
    "        Y[count] = -1\n",
    "    else:\n",
    "        Y[count] = 1\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = Logistic_Regression()\n",
    "logit.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = logit.gradient_descent_logistic(0.001, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.31900196, -0.37231203,  0.01426821,  0.19981285, -0.22381852,\n",
       "         0.16420569, -0.59156683, -0.48309589,  0.19216084,  0.46322814]),\n",
       " -0.07960530546470693)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat = np.array(w.dot(z_standardize(X).T) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33228718, 0.2322121 , 0.28437267, ..., 0.70643604, 0.67193117,\n",
       "       0.71691831])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hat1 = sigmoid(hat)\n",
    "hat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  10,   15,   17,   18,   19,   24,   25,   26,   29,   30,   35,\n",
       "          38,   45,   48,   49,   50,   52,   56,   57,   58,   61,   62,\n",
       "          64,   72,   75,   83,   89,   94,  110,  112,  132,  143,  150,\n",
       "         153,  156,  157,  158,  159,  160,  167,  178,  182,  183,  185,\n",
       "         186,  189,  192,  218,  219,  220,  224,  226,  228,  229,  231,\n",
       "         233,  234,  235,  239,  242,  252,  260,  263,  264,  277,  278,\n",
       "         279,  283,  292,  305,  313,  317,  320,  329,  350,  352,  353,\n",
       "         384,  386,  388,  394,  399,  400,  401,  402,  406,  407,  408,\n",
       "         420,  421,  428,  430,  434,  437,  441,  442,  443,  444,  447,\n",
       "         448,  449,  451,  452,  454,  455,  459,  461,  463,  464,  468,\n",
       "         469,  471,  472,  477,  478,  479,  480,  485,  486,  491,  493,\n",
       "         497,  500,  504,  506,  509,  514,  515,  518,  523,  525,  527,\n",
       "         530,  531,  532,  536,  538,  539,  540,  543,  546,  555,  560,\n",
       "         561,  562,  563,  564,  567,  572,  573,  602,  603,  604,  609,\n",
       "         611,  612,  615,  616,  617,  618,  620,  623,  624,  625,  627,\n",
       "         628,  631,  632,  633,  635,  636,  637,  638,  639,  645,  647,\n",
       "         648,  650,  651,  652,  653,  655,  656,  657,  658,  674,  675,\n",
       "         676,  679,  680,  681,  683,  684,  685,  691,  692,  694,  695,\n",
       "         698,  699,  700,  704,  706,  707,  714,  716,  717,  718,  719,\n",
       "         720,  722,  724,  725,  727,  729,  730,  732,  734,  736,  738,\n",
       "         739,  740,  742,  749,  751,  753,  754,  755,  756,  757,  758,\n",
       "         759,  760,  761,  762,  763,  764,  765,  766,  768,  769,  771,\n",
       "         775,  779,  783,  784,  785,  786,  787,  789,  791,  792,  793,\n",
       "         794,  796,  797,  799,  802,  803,  804,  805,  806,  807,  808,\n",
       "         809,  810,  812,  813,  814,  815,  819,  820,  823,  826,  827,\n",
       "         828,  830,  831,  835,  836,  838,  839,  840,  843,  844,  847,\n",
       "         848,  849,  850,  851,  852,  853,  854,  855,  857,  859,  862,\n",
       "         869,  870,  871,  872,  873,  874,  875,  876,  878,  879,  880,\n",
       "         882,  883,  884,  885,  887,  888,  889,  893,  898,  899,  900,\n",
       "         902,  903,  906,  907,  916,  919,  920,  923,  924,  926,  927,\n",
       "         928,  930,  932,  933,  934,  935,  937,  940,  943,  944,  945,\n",
       "         950,  951,  953,  954,  963,  964,  972,  973,  974,  976,  977,\n",
       "         978,  979,  981,  982,  983,  984,  985,  987,  988,  989,  990,\n",
       "         992,  993,  994,  995,  996,  997,  999, 1000, 1003, 1004, 1006,\n",
       "        1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017,\n",
       "        1018, 1019, 1020, 1021, 1022, 1024, 1025, 1026, 1027, 1028, 1029,\n",
       "        1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040,\n",
       "        1041, 1044, 1045, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1055,\n",
       "        1056, 1057, 1059, 1060, 1061, 1062, 1063, 1065, 1066, 1069, 1070,\n",
       "        1071, 1072, 1073, 1076, 1077, 1079, 1081, 1082, 1083, 1086, 1087,\n",
       "        1088, 1089, 1090, 1091, 1093, 1094, 1095, 1096, 1097, 1098, 1099,\n",
       "        1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110,\n",
       "        1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123,\n",
       "        1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134,\n",
       "        1135, 1136, 1137, 1138, 1142, 1143, 1145, 1146, 1147, 1148, 1149,\n",
       "        1150, 1152, 1153, 1154, 1155, 1156, 1158, 1159, 1160, 1161, 1162,\n",
       "        1163, 1164, 1165, 1166, 1167, 1168, 1170, 1171, 1172, 1173, 1174,\n",
       "        1175, 1177, 1178, 1179, 1183, 1184, 1187, 1188, 1189, 1190, 1191,\n",
       "        1192, 1193, 1195, 1196, 1200, 1201, 1204, 1205, 1206, 1207, 1208,\n",
       "        1212, 1213, 1214, 1215, 1217, 1218, 1219, 1220, 1221, 1222, 1223,\n",
       "        1226, 1229, 1230, 1231, 1233, 1234, 1236, 1237, 1238, 1240, 1241,\n",
       "        1242, 1243, 1244, 1245, 1247, 1254, 1255, 1257, 1260, 1261, 1262,\n",
       "        1263, 1264, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274,\n",
       "        1277, 1278, 1279, 1280, 1281, 1282, 1284, 1285, 1286, 1287, 1288,\n",
       "        1289, 1291, 1292, 1293, 1296, 1297, 1298, 1299, 1300, 1301, 1302,\n",
       "        1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1312, 1313, 1314,\n",
       "        1315, 1316, 1317, 1318]),)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(hat1 >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "921"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(hat)):\n",
    "    if hat1[i] < 0.5:\n",
    "        if Y[i] == 0:\n",
    "            count += 1\n",
    "    else:\n",
    "        if Y[i] == 1:\n",
    "            count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7035633055344959"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "928/1319"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
