{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression():\n",
    "    def __init__(self, alpha = 1e-10 , num_iter = 10000, early_stop = 1e-50, intercept = True, init_weight = None):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "            Some initializations, if neccesary\n",
    "            \n",
    "            attributes: \n",
    "                        alpha: Learning Rate, default 1e-10\n",
    "                        num_iter: Number of Iterations to update coefficient with training data\n",
    "                        early_stop: Constant control early_stop.\n",
    "                        intercept: Bool, If we are going to fit a intercept, default True.\n",
    "                        init_weight: Matrix (n x 1), input init_weight for testing.\n",
    "                        \n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.model_name = 'Linear Regression'\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.num_iter = num_iter\n",
    "        self.early_stop = early_stop\n",
    "        self.intercept = intercept\n",
    "        self.init_weight = init_weight  ### For testing correctness.\n",
    "        \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "            Save the datasets in our model, and perform gradient descent.\n",
    "            \n",
    "            Parameter:\n",
    "                X_train: Matrix or 2-D array. Input feature matrix.\n",
    "                Y_train: Matrix or 2-D array. Input target value.\n",
    "                \n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        self.X = np.mat(X_train)\n",
    "        self.y = np.mat(y_train).T\n",
    "        \n",
    "        ### To be used when data are normalized\n",
    "        #self.min_target = min(y_train)\n",
    "        #self.max_target = max(y_train)\n",
    "        #self.y = min_max_normaliz(self.y)\n",
    "        \n",
    "        if self.intercept:\n",
    "            ones = np.ones((self.X.shape[0], 1))\n",
    "            self.X = np.hstack((ones, self.X))  # add a column of 1s at the beginning\n",
    "            \n",
    "        self.coef = np.random.uniform(-1, 1, size=(self.X.shape[1], 1))  # initialize coef with uniform distribution\n",
    "        \n",
    "        self.gradient_descent()  # call the gradient_descent function to train\n",
    "        \n",
    "        #self.coef = self.init_weight #### Please change this after you get the example right.\n",
    "        \n",
    "    def gradient(self):\n",
    "        \"\"\"\n",
    "            Helper function to calculate the gradient respect to coefficient.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.grad_coef = np.dot(self.X.T, np.dot(self.X, self.coef) - self.y) / self.X.shape[0]\n",
    "        \n",
    "    def gradient_descent(self):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "            Training function\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.loss = []\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "\n",
    "            self.gradient()\n",
    "            \n",
    "            previous_y_hat = self.X.dot(self.coef)\n",
    "            pre_error = np.mean(np.square(previous_y_hat - self.y))\n",
    "            \n",
    "            temp_coef = self.coef - self.alpha * self.grad_coef\n",
    "            \n",
    "            current_y_hat = self.X.dot(temp_coef)\n",
    "            current_error = np.mean(np.square(current_y_hat - self.y))\n",
    "            \n",
    "            if current_error <= pre_error:\n",
    "                self.alpha *= 1.3\n",
    "                self.coef = temp_coef\n",
    "            else:\n",
    "                self.alpha *= 0.9\n",
    "                \n",
    "            self.loss.append(current_error)\n",
    "            \n",
    "            ### This is the early stop, don't modify following three lines.\n",
    "            if (abs(pre_error - current_error) < self.early_stop) | (abs(abs(pre_error - current_error) / pre_error) < self.early_stop):\n",
    "                self.coef = temp_coef\n",
    "                print(f'Iteration = {i}')\n",
    "                return self\n",
    "                \n",
    "            if i % 10000 == 0:\n",
    "                print('Iteration: ' +  str(i))\n",
    "                print('Coef: '+ str(self.coef))\n",
    "                print('Loss: ' + str(current_error))\n",
    "                \n",
    "        return self\n",
    "\n",
    "    \n",
    "    def ind_predict(self, x: list):\n",
    "        \"\"\"\n",
    "            Predict the value based on its feature vector x.\n",
    "\n",
    "            Parameter:\n",
    "            x: Matrix, array or list. Input feature point.\n",
    "            \n",
    "            Return:\n",
    "                result: prediction of given data point\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"    \n",
    "        x = np.hstack((np.ones((1,1)), np.mat(x))) if self.intercept else np.mat(x)\n",
    "        result = x.dot(self.coef)\n",
    "        return result\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        result = np.dot(x, self.coef)\n",
    "        return result\n",
    "        \"\"\"\n",
    "        x = np.mat(x).T\n",
    "        result = float(x.T * self.coef)\n",
    "        return result\n",
    "\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Parameter:\n",
    "            X: Matrix, array or list. Input feature point.\n",
    "            \n",
    "            Return:\n",
    "                ret: prediction of given data matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        ret = []\n",
    "        X = np.mat(X)\n",
    "\n",
    "        if self.intercept:\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((ones, X))\n",
    "\n",
    "        for x in X:\n",
    "            pred = self.ind_predict(x)\n",
    "            ### To be used when data are normalized\n",
    "            #pred =  pred * (self.max_target - self.min_target) + self.min_target\n",
    "            ret.append(pred)\n",
    "\n",
    "        return ret\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normaliz(lst):\n",
    "    \"\"\"\n",
    "    Helper function for normalize for faster training.\n",
    "    \"\"\"\n",
    "    maximum = np.max(lst)\n",
    "    minimum = np.min(lst)\n",
    "\n",
    "    return (lst - minimum) / (maximum - minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We generate some easy data for testing. We should fit a line with, $Y = 30 * X + 20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(np.mat(np.arange(1, 1000, 5)).T)\n",
    "y = np.array((30 * X)).flatten() +  20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do NOT modify the following line, just run it when you are done.  You can also try different initialization, you will notice different coef at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Coef: [[ 0.58701559]\n",
      " [-0.94003496]]\n",
      "Loss: 3.5044696913028385e+21\n",
      "Iteration: 10000\n",
      "Coef: [[ 0.71613063]\n",
      " [30.02903548]]\n",
      "Loss: 93.38825416199975\n",
      "Iteration: 20000\n",
      "Coef: [[ 0.79965782]\n",
      " [30.02877068]]\n",
      "Loss: 92.58090742761162\n",
      "Iteration: 30000\n",
      "Coef: [[ 0.88282014]\n",
      " [30.02878589]]\n",
      "Loss: 91.7793963244437\n",
      "Iteration: 40000\n",
      "Coef: [[ 0.96565234]\n",
      " [30.02866208]]\n",
      "Loss: 90.98522893288757\n",
      "Iteration: 50000\n",
      "Coef: [[ 1.048096  ]\n",
      " [30.02840573]]\n",
      "Loss: 90.19866371152816\n",
      "Iteration: 60000\n",
      "Coef: [[ 1.13018272]\n",
      " [30.02841018]]\n",
      "Loss: 89.4207343576167\n",
      "Iteration: 70000\n",
      "Coef: [[ 1.21191364]\n",
      " [30.02816516]]\n",
      "Loss: 88.6468132898496\n",
      "Iteration: 80000\n",
      "Coef: [[ 1.29329389]\n",
      " [30.02817106]]\n",
      "Loss: 87.88066182391543\n",
      "Iteration: 90000\n",
      "Coef: [[ 1.37431832]\n",
      " [30.02791727]]\n",
      "Loss: 87.12018684683369\n",
      "Iteration: 100000\n",
      "Coef: [[ 1.45502219]\n",
      " [30.02778977]]\n",
      "Loss: 86.36710981548184\n",
      "Iteration: 110000\n",
      "Coef: [[ 1.53534409]\n",
      " [30.02779415]]\n",
      "Loss: 85.62135649815387\n",
      "Iteration: 120000\n",
      "Coef: [[ 1.61532372]\n",
      " [30.02755272]]\n",
      "Loss: 84.88155973341998\n",
      "Iteration: 130000\n",
      "Coef: [[ 1.69495398]\n",
      " [30.02756334]]\n",
      "Loss: 84.14712170109736\n",
      "Iteration: 140000\n",
      "Coef: [[ 1.77423881]\n",
      " [30.027317  ]]\n",
      "Loss: 83.41919608145236\n",
      "Iteration: 150000\n",
      "Coef: [[ 1.85321027]\n",
      " [30.02718997]]\n",
      "Loss: 82.6982123033968\n",
      "Iteration: 160000\n",
      "Coef: [[ 1.93180776]\n",
      " [30.0272027 ]]\n",
      "Loss: 81.98452300402602\n",
      "Iteration: 170000\n",
      "Coef: [[ 2.01006765]\n",
      " [30.02696629]]\n",
      "Loss: 81.27520143149884\n",
      "Iteration: 180000\n",
      "Coef: [[ 2.08799147]\n",
      " [30.02698226]]\n",
      "Loss: 80.57315917456714\n",
      "Iteration: 190000\n",
      "Coef: [[ 2.16557165]\n",
      " [30.02673242]]\n",
      "Loss: 79.87529886198833\n",
      "Iteration: 200000\n",
      "Coef: [[ 2.24284791]\n",
      " [30.02660637]]\n",
      "Loss: 79.1850297073278\n",
      "Iteration: 210000\n",
      "Coef: [[ 2.31975501]\n",
      " [30.02661039]]\n",
      "Loss: 78.50090523839872\n",
      "Iteration: 220000\n",
      "Coef: [[ 2.39633775]\n",
      " [30.02638155]]\n",
      "Loss: 77.82295230262477\n",
      "Iteration: 230000\n",
      "Coef: [[ 2.47258615]\n",
      " [30.02639659]]\n",
      "Loss: 77.14980862842796\n",
      "Iteration: 240000\n",
      "Coef: [[ 2.54852875]\n",
      " [30.02627851]]\n",
      "Loss: 76.48195393107117\n",
      "Iteration: 250000\n",
      "Coef: [[ 2.62411811]\n",
      " [30.02603524]]\n",
      "Loss: 75.82110828907557\n",
      "Iteration: 260000\n",
      "Coef: [[ 2.69937495]\n",
      " [30.02603811]]\n",
      "Loss: 75.16596281983578\n",
      "Iteration: 270000\n",
      "Coef: [[ 2.77431348]\n",
      " [30.02581281]]\n",
      "Loss: 74.51708567572922\n",
      "Iteration: 280000\n",
      "Coef: [[ 2.84892495]\n",
      " [30.02583544]]\n",
      "Loss: 73.87273842277374\n",
      "Iteration: 290000\n",
      "Coef: [[ 2.92321035]\n",
      " [30.02559417]]\n",
      "Loss: 73.23307419386914\n",
      "Iteration: 300000\n",
      "Coef: [[ 2.99720489]\n",
      " [30.02547048]]\n",
      "Loss: 72.60033737624835\n",
      "Iteration: 310000\n",
      "Coef: [[ 3.0708462 ]\n",
      " [30.02548512]]\n",
      "Loss: 71.97328553078349\n",
      "Iteration: 320000\n",
      "Coef: [[ 3.14417368]\n",
      " [30.02526346]]\n",
      "Loss: 71.35104937548498\n",
      "Iteration: 330000\n",
      "Coef: [[ 3.21718371]\n",
      " [30.02527651]]\n",
      "Loss: 70.73418219214541\n",
      "Iteration: 340000\n",
      "Coef: [[ 3.28987424]\n",
      " [30.02504516]]\n",
      "Loss: 70.12196378285572\n",
      "Iteration: 350000\n",
      "Coef: [[ 3.36227763]\n",
      " [30.02493518]]\n",
      "Loss: 69.51565692923175\n",
      "Iteration: 360000\n",
      "Coef: [[ 3.43434074]\n",
      " [30.02494157]]\n",
      "Loss: 68.91605530142787\n",
      "Iteration: 370000\n",
      "Coef: [[ 3.50609407]\n",
      " [30.02471453]]\n",
      "Loss: 68.3204428880232\n",
      "Iteration: 380000\n",
      "Coef: [[ 3.57753418]\n",
      " [30.02472978]]\n",
      "Loss: 67.72900660465798\n",
      "Iteration: 390000\n",
      "Coef: [[ 3.64869095]\n",
      " [30.02462208]]\n",
      "Loss: 67.14288333842612\n",
      "Iteration: 400000\n",
      "Coef: [[ 3.71951654]\n",
      " [30.02438899]]\n",
      "Loss: 66.5629295021068\n",
      "Iteration: 410000\n",
      "Coef: [[ 3.79003025]\n",
      " [30.02440519]]\n",
      "Loss: 65.988281527519\n",
      "Iteration: 420000\n",
      "Coef: [[ 3.86024363]\n",
      " [30.02418447]]\n",
      "Loss: 65.41799484880825\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-674e8513844e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinear_Regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_weight\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-502236413750>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# initialize coef with uniform distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# call the gradient_descent function to train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m#self.coef = self.init_weight #### Please change this after you get the example right.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-502236413750>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mcurrent_y_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_coef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mcurrent_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_y_hat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_error\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mpre_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3368\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3370\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3372\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = Linear_Regression(alpha = 10, num_iter = 1000000, init_weight= np.mat([15,25]).T)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  As the number of iteration increase, you should notice the coeficient converges to [20, 30]. \n",
    "#### It maybe very slow update. Feel free to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.array(clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please try to normalize the X and fit again with normalized X. You should find something interesting. Also think about what you should do for predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = min_max_normaliz(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Coef: [[-0.90158397]\n",
      " [-0.70814329]]\n",
      "Loss: 3.3279164328676285\n",
      "Iteration = 1351\n"
     ]
    }
   ],
   "source": [
    "clf_norm = Linear_Regression(num_iter = 100000, init_weight= np.mat([15,25]).T)\n",
    "clf_norm.fit(X_norm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1.43949362e-23],\n",
       "        [1.00000000e+00]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_norm.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   50.,   200.,   350.,   500.,   650.,   800.,   950.,  1100.,\n",
       "        1250.,  1400.,  1550.,  1700.,  1850.,  2000.,  2150.,  2300.,\n",
       "        2450.,  2600.,  2750.,  2900.,  3050.,  3200.,  3350.,  3500.,\n",
       "        3650.,  3800.,  3950.,  4100.,  4250.,  4400.,  4550.,  4700.,\n",
       "        4850.,  5000.,  5150.,  5300.,  5450.,  5600.,  5750.,  5900.,\n",
       "        6050.,  6200.,  6350.,  6500.,  6650.,  6800.,  6950.,  7100.,\n",
       "        7250.,  7400.,  7550.,  7700.,  7850.,  8000.,  8150.,  8300.,\n",
       "        8450.,  8600.,  8750.,  8900.,  9050.,  9200.,  9350.,  9500.,\n",
       "        9650.,  9800.,  9950., 10100., 10250., 10400., 10550., 10700.,\n",
       "       10850., 11000., 11150., 11300., 11450., 11600., 11750., 11900.,\n",
       "       12050., 12200., 12350., 12500., 12650., 12800., 12950., 13100.,\n",
       "       13250., 13400., 13550., 13700., 13850., 14000., 14150., 14300.,\n",
       "       14450., 14600., 14750., 14900., 15050., 15200., 15350., 15500.,\n",
       "       15650., 15800., 15950., 16100., 16250., 16400., 16550., 16700.,\n",
       "       16850., 17000., 17150., 17300., 17450., 17600., 17750., 17900.,\n",
       "       18050., 18200., 18350., 18500., 18650., 18800., 18950., 19100.,\n",
       "       19250., 19400., 19550., 19700., 19850., 20000., 20150., 20300.,\n",
       "       20450., 20600., 20750., 20900., 21050., 21200., 21350., 21500.,\n",
       "       21650., 21800., 21950., 22100., 22250., 22400., 22550., 22700.,\n",
       "       22850., 23000., 23150., 23300., 23450., 23600., 23750., 23900.,\n",
       "       24050., 24200., 24350., 24500., 24650., 24800., 24950., 25100.,\n",
       "       25250., 25400., 25550., 25700., 25850., 26000., 26150., 26300.,\n",
       "       26450., 26600., 26750., 26900., 27050., 27200., 27350., 27500.,\n",
       "       27650., 27800., 27950., 28100., 28250., 28400., 28550., 28700.,\n",
       "       28850., 29000., 29150., 29300., 29450., 29600., 29750., 29900.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(clf_norm.predict(X_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   50,   200,   350,   500,   650,   800,   950,  1100,  1250,\n",
       "        1400,  1550,  1700,  1850,  2000,  2150,  2300,  2450,  2600,\n",
       "        2750,  2900,  3050,  3200,  3350,  3500,  3650,  3800,  3950,\n",
       "        4100,  4250,  4400,  4550,  4700,  4850,  5000,  5150,  5300,\n",
       "        5450,  5600,  5750,  5900,  6050,  6200,  6350,  6500,  6650,\n",
       "        6800,  6950,  7100,  7250,  7400,  7550,  7700,  7850,  8000,\n",
       "        8150,  8300,  8450,  8600,  8750,  8900,  9050,  9200,  9350,\n",
       "        9500,  9650,  9800,  9950, 10100, 10250, 10400, 10550, 10700,\n",
       "       10850, 11000, 11150, 11300, 11450, 11600, 11750, 11900, 12050,\n",
       "       12200, 12350, 12500, 12650, 12800, 12950, 13100, 13250, 13400,\n",
       "       13550, 13700, 13850, 14000, 14150, 14300, 14450, 14600, 14750,\n",
       "       14900, 15050, 15200, 15350, 15500, 15650, 15800, 15950, 16100,\n",
       "       16250, 16400, 16550, 16700, 16850, 17000, 17150, 17300, 17450,\n",
       "       17600, 17750, 17900, 18050, 18200, 18350, 18500, 18650, 18800,\n",
       "       18950, 19100, 19250, 19400, 19550, 19700, 19850, 20000, 20150,\n",
       "       20300, 20450, 20600, 20750, 20900, 21050, 21200, 21350, 21500,\n",
       "       21650, 21800, 21950, 22100, 22250, 22400, 22550, 22700, 22850,\n",
       "       23000, 23150, 23300, 23450, 23600, 23750, 23900, 24050, 24200,\n",
       "       24350, 24500, 24650, 24800, 24950, 25100, 25250, 25400, 25550,\n",
       "       25700, 25850, 26000, 26150, 26300, 26450, 26600, 26750, 26900,\n",
       "       27050, 27200, 27350, 27500, 27650, 27800, 27950, 28100, 28250,\n",
       "       28400, 28550, 28700, 28850, 29000, 29150, 29300, 29450, 29600,\n",
       "       29750, 29900])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You can also try this with the wine dataset we use in HW1. Try fit this function to that dataset with same features. If you look closely to the updates of coefficients. What do you find? This could be mentioned in your report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine = pd.read_csv(url_Wine, delimiter=';')\n",
    "X = wine[['density','alcohol']]\n",
    "y = wine.quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800.6676988774342"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X,y)\n",
    "## Squared Error with sklearn.\n",
    "sum((lr.predict(X) - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34.82170159,  0.39144139])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-33.15237986168703"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>density</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.99780</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.99680</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.99700</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.99800</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.99780</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>0.99490</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>0.99512</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>0.99574</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>0.99547</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>0.99549</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      density  alcohol\n",
       "0     0.99780      9.4\n",
       "1     0.99680      9.8\n",
       "2     0.99700      9.8\n",
       "3     0.99800      9.8\n",
       "4     0.99780      9.4\n",
       "...       ...      ...\n",
       "1594  0.99490     10.5\n",
       "1595  0.99512     11.2\n",
       "1596  0.99574     11.0\n",
       "1597  0.99547     10.2\n",
       "1598  0.99549     11.0\n",
       "\n",
       "[1599 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will notice different coefficients, but the loss is very close to each other like 805. In your report, briefly discuss this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Coef: [[-0.45934637]\n",
      " [ 0.59925034]\n",
      " [ 0.8261422 ]]\n",
      "Loss: 121442.11329671436\n",
      "Iteration: 10000\n",
      "Coef: [[0.08743834]\n",
      " [1.15426921]\n",
      " [0.42102826]]\n",
      "Loss: 0.5078543356240083\n",
      "Iteration: 20000\n",
      "Coef: [[0.28883188]\n",
      " [1.3615057 ]\n",
      " [0.3827437 ]]\n",
      "Loss: 0.5042234605400849\n",
      "Iteration: 30000\n",
      "Coef: [[0.35785727]\n",
      " [1.43555273]\n",
      " [0.36899584]]\n",
      "Loss: 0.5037768505123259\n",
      "Iteration: 40000\n",
      "Coef: [[0.38055945]\n",
      " [1.46298858]\n",
      " [0.364308  ]]\n",
      "Loss: 0.5037217650511678\n",
      "Iteration: 50000\n",
      "Coef: [[0.38701903]\n",
      " [1.47408254]\n",
      " [0.36263455]]\n",
      "Loss: 0.5037146595696798\n",
      "Iteration: 60000\n",
      "Coef: [[0.38778835]\n",
      " [1.47944907]\n",
      " [0.36204398]]\n",
      "Loss: 0.5037134306218052\n",
      "Iteration: 70000\n",
      "Coef: [[0.38656696]\n",
      " [1.48281291]\n",
      " [0.36184756]]\n",
      "Loss: 0.5037129214526338\n",
      "Iteration: 80000\n",
      "Coef: [[0.38464677]\n",
      " [1.4854731 ]\n",
      " [0.36177256]]\n",
      "Loss: 0.5037124999137044\n",
      "Iteration: 90000\n",
      "Coef: [[0.38248321]\n",
      " [1.48788826]\n",
      " [0.36175461]]\n",
      "Loss: 0.5037120898900128\n",
      "Iteration: 100000\n",
      "Coef: [[0.38023293]\n",
      " [1.49021739]\n",
      " [0.36174789]]\n",
      "Loss: 0.5037116813371492\n",
      "Iteration: 110000\n",
      "Coef: [[0.37795317]\n",
      " [1.49251488]\n",
      " [0.36174198]]\n",
      "Loss: 0.5037112737325178\n",
      "Iteration: 120000\n",
      "Coef: [[0.37566385]\n",
      " [1.49480278]\n",
      " [0.36174816]]\n",
      "Loss: 0.5037108661081775\n",
      "Iteration: 130000\n",
      "Coef: [[0.37337017]\n",
      " [1.49708561]\n",
      " [0.36174426]]\n",
      "Loss: 0.50371045724555\n",
      "Iteration: 140000\n",
      "Coef: [[0.37107506]\n",
      " [1.49936838]\n",
      " [0.36174631]]\n",
      "Loss: 0.5037100487120599\n",
      "Iteration: 150000\n",
      "Coef: [[0.36878086]\n",
      " [1.50165028]\n",
      " [0.36175349]]\n",
      "Loss: 0.5037096409155373\n",
      "Iteration: 160000\n",
      "Coef: [[0.36648576]\n",
      " [1.50393078]\n",
      " [0.36175027]]\n",
      "Loss: 0.5037092338560679\n",
      "Iteration: 170000\n",
      "Coef: [[0.36419167]\n",
      " [1.50621216]\n",
      " [0.36175754]]\n",
      "Loss: 0.5037088259115919\n",
      "Iteration: 180000\n",
      "Coef: [[0.36189682]\n",
      " [1.50849229]\n",
      " [0.3617544 ]]\n",
      "Loss: 0.5037084174449294\n",
      "Iteration: 190000\n",
      "Coef: [[0.359603  ]\n",
      " [1.51077331]\n",
      " [0.36176146]]\n",
      "Loss: 0.503708009590532\n",
      "Iteration: 200000\n",
      "Coef: [[0.3573081 ]\n",
      " [1.51305446]\n",
      " [0.36176362]]\n",
      "Loss: 0.5037076018341313\n",
      "Iteration: 210000\n",
      "Coef: [[0.35501369]\n",
      " [1.51533412]\n",
      " [0.36176039]]\n",
      "Loss: 0.5037071950977218\n",
      "Iteration: 220000\n",
      "Coef: [[0.35272033]\n",
      " [1.51761472]\n",
      " [0.36176773]]\n",
      "Loss: 0.5037067874297141\n",
      "Iteration: 230000\n",
      "Coef: [[0.35042623]\n",
      " [1.51989404]\n",
      " [0.36176437]]\n",
      "Loss: 0.5037063793626405\n",
      "Iteration: 240000\n",
      "Coef: [[0.34813195]\n",
      " [1.52217455]\n",
      " [0.3617664 ]]\n",
      "Loss: 0.5037059714744193\n",
      "Iteration: 250000\n",
      "Coef: [[0.34583909]\n",
      " [1.52445472]\n",
      " [0.36177407]]\n",
      "Loss: 0.5037055643260754\n",
      "Iteration: 260000\n",
      "Coef: [[0.34354552]\n",
      " [1.52673352]\n",
      " [0.36177072]]\n",
      "Loss: 0.5037051574375809\n",
      "Iteration: 270000\n",
      "Coef: [[0.34125292]\n",
      " [1.52901332]\n",
      " [0.36177783]]\n",
      "Loss: 0.50370475022139\n",
      "Iteration: 280000\n",
      "Coef: [[0.33895959]\n",
      " [1.53129189]\n",
      " [0.36177458]]\n",
      "Loss: 0.5037043424560519\n",
      "Iteration: 290000\n",
      "Coef: [[0.33666608]\n",
      " [1.53357164]\n",
      " [0.36177657]]\n",
      "Loss: 0.5037039348872465\n",
      "Iteration: 300000\n",
      "Coef: [[0.33437404]\n",
      " [1.53585091]\n",
      " [0.36178379]]\n",
      "Loss: 0.5037035277645759\n",
      "Iteration: 310000\n",
      "Coef: [[0.33208131]\n",
      " [1.538129  ]\n",
      " [0.36178113]]\n",
      "Loss: 0.5037031210816766\n",
      "Iteration: 320000\n",
      "Coef: [[0.32978945]\n",
      " [1.54040798]\n",
      " [0.36178779]]\n",
      "Loss: 0.50370271425875\n",
      "Iteration: 330000\n",
      "Coef: [[0.32749692]\n",
      " [1.54268581]\n",
      " [0.36178482]]\n",
      "Loss: 0.5037023069259675\n",
      "Iteration: 340000\n",
      "Coef: [[0.32520417]\n",
      " [1.54496477]\n",
      " [0.36178665]]\n",
      "Loss: 0.5037018997584282\n",
      "Iteration: 350000\n",
      "Coef: [[0.32291295]\n",
      " [1.54724329]\n",
      " [0.36179419]]\n",
      "Loss: 0.5037014930262562\n",
      "Iteration: 360000\n",
      "Coef: [[0.32062094]\n",
      " [1.54952057]\n",
      " [0.36179104]]\n",
      "Loss: 0.5037010867833452\n",
      "Iteration: 370000\n",
      "Coef: [[0.31832992]\n",
      " [1.55179884]\n",
      " [0.36179838]]\n",
      "Loss: 0.5037006804724677\n",
      "Iteration: 380000\n",
      "Coef: [[0.31603821]\n",
      " [1.55407578]\n",
      " [0.36179498]]\n",
      "Loss: 0.503700272859452\n",
      "Iteration: 390000\n",
      "Coef: [[0.31374622]\n",
      " [1.55635396]\n",
      " [0.36179676]]\n",
      "Loss: 0.5036998660033624\n",
      "Iteration: 400000\n",
      "Coef: [[0.31145575]\n",
      " [1.55863174]\n",
      " [0.3618043 ]]\n",
      "Loss: 0.5036994594957195\n",
      "Iteration: 410000\n",
      "Coef: [[0.30916451]\n",
      " [1.56090826]\n",
      " [0.36180116]]\n",
      "Loss: 0.5036990535801721\n",
      "Iteration: 420000\n",
      "Coef: [[0.30687427]\n",
      " [1.56318578]\n",
      " [0.36180864]]\n",
      "Loss: 0.5036986476697842\n",
      "Iteration: 430000\n",
      "Coef: [[0.30458331]\n",
      " [1.56546193]\n",
      " [0.36180498]]\n",
      "Loss: 0.5036982403121251\n",
      "Iteration: 440000\n",
      "Coef: [[0.30229345]\n",
      " [1.56773904]\n",
      " [0.36181227]]\n",
      "Loss: 0.5036978337335697\n",
      "Iteration: 450000\n",
      "Coef: [[0.30000247]\n",
      " [1.57001626]\n",
      " [0.36181421]]\n",
      "Loss: 0.503697427245676\n",
      "Iteration: 460000\n",
      "Coef: [[0.29771203]\n",
      " [1.57229205]\n",
      " [0.36181143]]\n",
      "Loss: 0.5036970215795579\n",
      "Iteration: 470000\n",
      "Coef: [[0.29542254]\n",
      " [1.57456879]\n",
      " [0.36181865]]\n",
      "Loss: 0.503696615925684\n",
      "Iteration: 480000\n",
      "Coef: [[0.29313236]\n",
      " [1.57684419]\n",
      " [0.36181515]]\n",
      "Loss: 0.5036962090025844\n",
      "Iteration: 490000\n",
      "Coef: [[0.29084204]\n",
      " [1.57912079]\n",
      " [0.3618173 ]]\n",
      "Loss: 0.5036958024632695\n"
     ]
    }
   ],
   "source": [
    "clf = Linear_Regression(alpha = 1, num_iter = 500000)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805.408939546096"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((clf.predict(X) - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
