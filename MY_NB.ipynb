{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4zcwLIJuG1O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_Bayes():\n",
    "    \"\"\"\n",
    "    \n",
    "    Naive Bayes classifer\n",
    "    \n",
    "    Attributes:\n",
    "        prior: P(Y)\n",
    "        likelihood: P(X_j | Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Some initializations, if neccesary\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model_name = 'Naive Bayes'\n",
    "    \n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\" \n",
    "        The fit function fits the Naive Bayes model based on the training data. \n",
    "        Here, we assume that all the features are **discrete** features. \n",
    "\n",
    "        X_train is a matrix or 2-D numpy array, represnting training instances. \n",
    "        Each training instance is a feature vector. \n",
    "\n",
    "        y_train contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        TODO: 1. Modify and add some codes to the following for-loop\n",
    "                 to compute the correct prior distribution of all y labels.\n",
    "              2. Make sure they are normalized to a distribution.\n",
    "        \"\"\"\n",
    "        self.classes = np.unique(y_train)\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.num_features = X_train.shape[1]\n",
    "        \n",
    "        # Compute prior distribution of all y labels\n",
    "        self.prior = {f'Y = {y}': (y_train == y).sum() for y in self.classes}\n",
    "        total = sum(self.prior.values())\n",
    "        self.total = total\n",
    "        self.prior = {k: v/total for k, v in self.prior.items()}\n",
    "        \n",
    "        \"\"\"\n",
    "            TODO: 3. Modify and add some codes to the following for-loops\n",
    "                     to compute the correct likelihood P(X_j | Y).\n",
    "                  4. Make sure they are normalized to distributions.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.likelihood = dict()\n",
    "        label_count = {f'{y}': (y_train == y).sum() for y in self.classes}\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            for j in range(x.shape[1]):\n",
    "                key = f'X{j} = {x[0,j]} | Y = {y}'\n",
    "                if key in self.likelihood:\n",
    "                    self.likelihood[key] += 1\n",
    "                else:\n",
    "                    self.likelihood[key] = 1\n",
    "    \n",
    "        for key in self.likelihood:\n",
    "            y = key.split('Y = ')[-1]\n",
    "            self.likelihood[key] /= label_count[y]\n",
    "        \n",
    "        return self.prior\n",
    "        #return self.likelihood\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "            TODO: 5. Think about whether we really need P(X_1 = x_1, X_2 = x_2, ..., X_d = x_d)\n",
    "                     in practice?\n",
    "                  6. Does this really matter for the final classification results?\n",
    "        \"\"\"\n",
    "        ### Answers in the report\n",
    "        \n",
    "    def ind_predict(self, x: list):\n",
    "        \"\"\" \n",
    "        Predict the most likely class label of one test instance based on its feature vector x.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "            TODO: 7. Enumerate all possible class labels and compute the likelihood \n",
    "                     based on the given feature vector x. Don't forget to incorporate \n",
    "                     both the prior and likelihood.\n",
    "                  8. Pick the label with the highest probability. \n",
    "                  9. How to deal with very small probability values, especially\n",
    "                     when the feature vector is of a high dimension. (Hint: log)\n",
    "                  10. How to how to deal with unknown feature values?\n",
    "        \"\"\"\n",
    "\n",
    "        ret, max_prob = None, -float('inf')\n",
    "        for y in self.prior.keys():\n",
    "            prob = np.log(self.prior[y])\n",
    "            for j in range(x.shape[1]):\n",
    "                # Check if the feature value is known\n",
    "                feature_name = f'X{j} = {x[0,j]} | {y}'\n",
    "                if feature_name in self.likelihood:\n",
    "                    prob += np.log(self.likelihood[feature_name])\n",
    "                    #print(f'{prob}')\n",
    "                else:\n",
    "                    # Apply smoothing for unknown feature values\n",
    "                    num_features = self.num_features\n",
    "                    prob *= 1 / (self.prior[y]*self.total + num_features)\n",
    "                    print('unknown')\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                y_1 = y.split('Y = ')[-1]\n",
    "                ret = y_1\n",
    "        return ret\n",
    "\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "        \"\"\"\n",
    "        X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "        Each testing instance is a feature vector. \n",
    "\n",
    "        Return the predictions of all instances in a list.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        TODO: 11. Revise the following for-loop to call ind_predict to get predictions.\n",
    "        \"\"\"\n",
    "\n",
    "        ret = []\n",
    "        for x in X:\n",
    "            # Use ind_predict method to predict class label for each instance x\n",
    "            pred = self.ind_predict(x)\n",
    "            ret.append(pred)\n",
    "\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0g4b0OM7uG1U"
   },
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data'\n",
    "col = ['class_name','left_weight','left_distance','right_weight','right_distance']\n",
    "data = pd.read_csv(url, delimiter = ',', names = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZAsYzCPzuG1Z",
    "outputId": "60ec9f98-d513-479d-a6cf-e65d50d2d3c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R    288\n",
       "L    288\n",
       "B     49\n",
       "Name: class_name, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.class_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0JjkLsxGuG1e"
   },
   "outputs": [],
   "source": [
    "X = np.matrix(data.iloc[:,1:])\n",
    "y = data.class_name\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L    202\n",
       "R    184\n",
       "B     32\n",
       "Name: class_name, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZxU7v9SxuG1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Y = B': 0.07655502392344497,\n",
       " 'Y = L': 0.48325358851674644,\n",
       " 'Y = R': 0.44019138755980863}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Naive_Bayes()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)\n",
    "y_hat = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'L',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'R',\n",
       " 'L']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2RZ2jYsuG1h"
   },
   "source": [
    "Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpfumdTCuG1i",
    "outputId": "6925cbf8-873b-49f0-80b2-754f569c3f8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8840579710144928"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_hat == y_test)/ 207  # you should get something like 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itazR0opuG1k",
    "outputId": "9efcc83c-1b5a-4a92-9373-07be8301d61b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAUQN8uFuG1l",
    "outputId": "5039b87d-c601-4a6f-9310-caf7e9283c5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
